{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import getpass\n",
    "import operator\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, fields\n",
    "from typing import Any, Dict, List, Optional\n",
    "from typing_extensions import TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from langsmith import traceable\n",
    "# IPython for display (if needed)\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# Import LangChain and related tools\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda, RunnableBranch\n",
    "from langchain_core.tools import tool, StructuredTool\n",
    "from langchain_core import tools  # if needed\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# For financial data via yfinance\n",
    "import yfinance as yf\n",
    "\n",
    "# For web search and HTML parsing\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# For environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# For YouTube video recommendations\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "\n",
    "# For state graph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from tavily import TavilyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name='Gemma2-9b-it', api_key=os.getenv('GROQ_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_writer_instruction_web = \"\"\"Your goal is to generate a targeted web search query related to financial investments or any finance-related topic specified by the user.\n",
    "\n",
    "<TOPIC>\n",
    "{finance_topic}\n",
    "</TOPIC>\n",
    "\n",
    "<FORMAT>\n",
    "Format your response as a JSON object with ALL three of these exact keys:\n",
    "   - \"query\": The actual search query string\n",
    "   - \"aspect\": The specific aspect of the finance topic being researched\n",
    "   - \"rationale\": Brief explanation of why this query is relevant\n",
    "</FORMAT>\n",
    "\n",
    "<EXAMPLE>\n",
    "Example output:\n",
    "{{\n",
    "    \"query\": \"best index funds for long-term investment 2025\",\n",
    "    \"aspect\": \"investment strategy\",\n",
    "    \"rationale\": \"Identifying top-performing index funds for long-term portfolio growth\"\n",
    "}}\n",
    "</EXAMPLE>\n",
    "\n",
    "Provide your response in JSON format:\n",
    "\"\"\"\n",
    "\n",
    "summarizer_instruction_web = \"\"\"<GOAL>\n",
    "Generate a high-quality summary of the web search results, focusing on financial investments or the specific finance-related topic requested by the user.\n",
    "</GOAL>\n",
    "\n",
    "<REQUIREMENTS>\n",
    "When creating a NEW summary:\n",
    "1. Highlight the most relevant financial insights, trends, or strategies from the search results.\n",
    "2. Ensure a coherent flow of information while keeping it concise and actionable.\n",
    "\n",
    "When EXTENDING an existing summary:\n",
    "1. Read the existing summary and new search results carefully.\n",
    "2. Compare the new information with the existing summary.\n",
    "3. For each piece of new information:\n",
    "    a. If it builds on an existing point, integrate it smoothly.\n",
    "    b. If it introduces a new relevant aspect, add a separate paragraph.\n",
    "    c. If it’s irrelevant to financial investments, ignore it.\n",
    "4. Ensure all additions align with the user’s finance-related query.\n",
    "5. Verify that the final output differs from the original summary while improving its depth.\n",
    "\n",
    "<FORMATTING>\n",
    "- Start directly with the updated summary, without preamble or titles. Do not use XML tags in the output.\n",
    "</FORMATTING>\n",
    "\"\"\"\n",
    "\n",
    "reflection_instructions_web = \"\"\"You are an expert financial research assistant analyzing a summary about {finance_topic}.\n",
    "\n",
    "<GOAL>\n",
    "1. Identify missing details or areas that need deeper exploration.\n",
    "2. Generate a follow-up question to help expand financial knowledge.\n",
    "3. Focus on investment strategies, market trends, risk factors, regulations, or financial instruments that weren’t fully covered.\n",
    "</GOAL>\n",
    "\n",
    "<REQUIREMENTS>\n",
    "Ensure the follow-up question is self-contained and provides necessary context for a web search.\n",
    "</REQUIREMENTS>\n",
    "\n",
    "<FORMAT>\n",
    "Format your response as a JSON object with these exact keys:\n",
    "- \"knowledge_gap\": Describe what financial information is missing or unclear.\n",
    "- \"follow_up_query\": Write a specific question to address this gap.\n",
    "</FORMAT>\n",
    "\n",
    "<EXAMPLE>\n",
    "Example output:\n",
    "{{\n",
    "    \"knowledge_gap\": \"The summary does not mention tax implications of investing in ETFs vs. mutual funds.\",\n",
    "    \"follow_up_query\": \"What are the tax advantages and disadvantages of ETFs compared to mutual funds?\"\n",
    "}}\n",
    "</EXAMPLE>\n",
    "\n",
    "Provide your analysis in JSON format:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    route: Literal['Web_query', 'Normal_query', 'Financial_Analysis', 'YouTube_Recommender'] = Field(None)\n",
    "    research_topic: str\n",
    "    search_query: str\n",
    "    web_research_results: List[str]\n",
    "    sources_gathered: List[str]\n",
    "    research_loop_count: int\n",
    "    running_summary: str\n",
    "    image: list[str]\n",
    "    image_processed: bool\n",
    "    messages: List[Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_state(user_query: str, image: list[str] = []) -> State:\n",
    "    return {\n",
    "        \"route\": None,\n",
    "        \"research_topic\": user_query,\n",
    "        \"search_query\": \"\",\n",
    "        \"web_research_results\": [],\n",
    "        \"sources_gathered\": [],\n",
    "        \"research_loop_count\": 0,\n",
    "        \"running_summary\": \"\",\n",
    "        \"image\": image,\n",
    "        \"image_processed\": False,\n",
    "        \"messages\": [HumanMessage(content=user_query)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchAPI(Enum):\n",
    "    PERPLEXITY = \"perplexity\"\n",
    "    TAVILY = \"tavily\"\n",
    "    DUCKDUCKGO = \"duckduckgo\"\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    max_web_research_loops: int = int(os.environ.get(\"MAX_WEB_RESEARCH_LOOPS\", \"3\"))\n",
    "    search_api: SearchAPI = SearchAPI(os.environ.get(\"SEARCH_API\", \"tavily\"))\n",
    "    fetch_full_page: bool = os.environ.get(\"FETCH_FULL_PAGE\", \"False\").lower() in (\"true\", \"1\", \"t\")\n",
    "    ollama_base_url: str = os.environ.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434/\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(cls, config: Optional[RunnableConfig] = None) -> \"Configuration\":\n",
    "        configurable = config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def tavily_search(query, include_raw_content=True, max_results=3):\n",
    "    api_key = os.environ['TAVILY_API_KEY']\n",
    "    if not api_key:\n",
    "        raise ValueError(\"TAVILY_API_KEY environment variable is not set\")\n",
    "    tavily_client = TavilyClient(api_key=api_key)\n",
    "    return tavily_client.search(query, max_results=max_results, include_raw_content=include_raw_content)\n",
    "\n",
    "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=False):\n",
    "    if isinstance(search_response, dict):\n",
    "        sources_list = search_response['results']\n",
    "    elif isinstance(search_response, list):\n",
    "        sources_list = []\n",
    "        for response in search_response:\n",
    "            if isinstance(response, dict) and 'results' in response:\n",
    "                sources_list.extend(response['results'])\n",
    "            else:\n",
    "                sources_list.extend(response)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
    "\n",
    "    unique_sources = {}\n",
    "    for source in sources_list:\n",
    "        if source['url'] not in unique_sources:\n",
    "            unique_sources[source['url']] = source\n",
    "\n",
    "    formatted_text = \"Sources:\\n\\n\"\n",
    "    for source in unique_sources.values():\n",
    "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
    "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
    "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
    "        if include_raw_content:\n",
    "            char_limit = max_tokens_per_source * 4\n",
    "            raw_content = source.get('raw_content', '') or ''\n",
    "            if len(raw_content) > char_limit:\n",
    "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
    "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
    "\n",
    "    return formatted_text.strip()\n",
    "\n",
    "def format_sources(search_results):\n",
    "    return '\\n'.join(\n",
    "        f\"* {source['title']} : {source['url']}\"\n",
    "        for source in search_results['results']\n",
    "    )\n",
    "\n",
    "def generate_query(state: State, config: RunnableConfig):\n",
    "    prompt = query_writer_instruction_web.format(finance_topic=state[\"research_topic\"]) + \"\\nGenerate a query for web search:\"\n",
    "    result = llm.invoke(prompt)\n",
    "    output_text = result.content.strip()\n",
    "    try:\n",
    "        query_data = json.loads(output_text)\n",
    "        return {\"search_query\": query_data['query']}\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {\"search_query\": f\"comprehensive analysis of {state['research_topic']}\"}\n",
    "\n",
    "def web_research(state: State, config: RunnableConfig):\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    search_api = configurable.search_api.value if isinstance(configurable.search_api, Enum) is False else configurable.search_api.value\n",
    "    if search_api == \"tavily\":\n",
    "        search_results = tavily_search(state[\"search_query\"], include_raw_content=True, max_results=1)\n",
    "        search_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported search API: {configurable.search_api}\")\n",
    "    return {\n",
    "        \"sources_gathered\": [format_sources(search_results)],\n",
    "        \"research_loop_count\": state[\"research_loop_count\"] + 1,\n",
    "        \"web_research_results\": [search_str]\n",
    "    }\n",
    "\n",
    "def summarize_sources(state: State, config: RunnableConfig):\n",
    "    existing_summary = state['running_summary']\n",
    "    most_recent_web_research = state['web_research_results'][-1]\n",
    "    if existing_summary:\n",
    "        human_message_content = (\n",
    "            f\"<User Input> \\n {state['research_topic']} \\n <User Input>\\n\\n\"\n",
    "            f\"<Existing Summary> \\n {existing_summary} \\n <Existing Summary>\\n\\n\"\n",
    "            f\"<New Search Results> \\n {most_recent_web_research} \\n <New Search Results>\"\n",
    "        )\n",
    "    else:\n",
    "        human_message_content = (\n",
    "            f\"<User Input> \\n {state['research_topic']} \\n <User Input>\\n\\n\"\n",
    "            f\"<Search Results> \\n {most_recent_web_research} \\n <Search Results>\"\n",
    "        )\n",
    "    prompt = summarizer_instruction_web + \"\\n\" + human_message_content\n",
    "    result = llm.invoke(prompt)\n",
    "    running_summary = result.content\n",
    "    while \"<think>\" in running_summary and \"</think>\" in running_summary:\n",
    "        start = running_summary.find(\"<think>\")\n",
    "        end = running_summary.find(\"</think>\") + len(\"</think>\")\n",
    "        running_summary = running_summary[:start] + running_summary[end:]\n",
    "    return {\"running_summary\": running_summary}\n",
    "\n",
    "def reflect_on_summary(state: State, config: RunnableConfig):\n",
    "    prompt = reflection_instructions_web.format(finance_topic=state['research_topic']) \\\n",
    "             + \"\\nIdentify a knowledge gap and generate a follow-up web search query based on our existing knowledge: \" \\\n",
    "             + state['running_summary']\n",
    "    result = llm.invoke(prompt)\n",
    "    output_text = result.content.strip()\n",
    "    try:\n",
    "        follow_up_query = json.loads(output_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Could not decode JSON from reflect_on_summary. Response was:\", output_text)\n",
    "        follow_up_query = {\"follow_up_query\": f\"Tell me more about {state['research_topic']}\"}\n",
    "    query = follow_up_query.get('follow_up_query')\n",
    "    if not query:\n",
    "        return {\"search_query\": f\"Tell me more about {state['research_topic']}\"}\n",
    "    return {\"search_query\": query}\n",
    "\n",
    "def finalize_summary(state: State):\n",
    "    all_sources = \"\\n\".join(source for source in state['sources_gathered'])\n",
    "    final_summary = f\"## Web Research Summary\\n\\n{state['running_summary']}\\n\\n### Sources:\\n{all_sources}\"\n",
    "    final_message = HumanMessage(content=final_summary)\n",
    "    return {\"running_summary\": final_summary, \"messages\": [final_message]}\n",
    "\n",
    "def route_research(state: State, config: RunnableConfig) -> Literal[\"finalize_summary\", \"web_research\"]:\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    if state['research_loop_count'] < configurable.max_web_research_loops:\n",
    "        return \"web_research\"\n",
    "    return \"finalize_summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(input_text):\n",
    "    parts = input_text.split(\"|||\")\n",
    "    query = parts[0]\n",
    "    response = parts[1]\n",
    "    sources = parts[2] if len(parts) > 2 else \"\"\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"\n",
    "    Evaluate the following response to the query:\n",
    "    \n",
    "    QUERY: {query}\n",
    "    RESPONSE: {response}\n",
    "    SOURCES: {sources}\n",
    "    \n",
    "    Assess based on:\n",
    "    1. Factual accuracy (Does it match the sources?)\n",
    "    2. Completeness (Does it address all aspects of the query?)\n",
    "    3. Relevance (Is the information relevant to the query?)\n",
    "    4. Hallucination (Does it contain information not supported by sources?)\n",
    "    \n",
    "    Return a confidence score from 0-10 and a brief explanation.\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation = llm.predict(evaluation_prompt)\n",
    "    return evaluation\n",
    "\n",
    "def evaluate_response(state: State, config: RunnableConfig):\n",
    "    query = state.get(\"research_topic\", \"\")\n",
    "    response = state.get(\"running_summary\", \"\")\n",
    "    sources = \"\\n\".join(state.get(\"sources_gathered\", [])) or \"No sources available\"\n",
    "    input_text = f\"{query}|||{response}|||{sources}\"\n",
    "    evaluation = self_evaluate(input_text)\n",
    "    final_summary = response + \"\\n\\n## Self Evaluation\\n\\n\" + evaluation\n",
    "    return {\"running_summary\": final_summary, \"messages\": [HumanMessage(content=final_summary)]}\n",
    "\n",
    "def evaluation_decision(state: State, config: RunnableConfig):\n",
    "    final_text = state.get(\"running_summary\", \"\")\n",
    "    prompt = f\"\"\"\n",
    "    The final output and self-evaluation are as follows:\n",
    "    {final_text}\n",
    "    \n",
    "    Based on the above, do you think additional insights should be added?\n",
    "    If yes, return a JSON object with the key \"next_route\" set to one of the following options:\n",
    "      - \"call_llm\" for additional financial analysis,\n",
    "      - \"web_research\" for further web research,\n",
    "      - \"answer_normal_query\" for more normal query insights,\n",
    "      #- \"parallel_branches\" to combine branches again.\n",
    "    If no additional insights are needed, return \"done\".\n",
    "    \n",
    "    For example:\n",
    "    {{\"next_route\": \"call_llm\"}}\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt)\n",
    "    output_text = result.content.strip()\n",
    "    try:\n",
    "        decision = json.loads(output_text)\n",
    "        next_route = decision.get(\"next_route\", \"done\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in evaluation_decision:\", e)\n",
    "        next_route = \"done\"\n",
    "    # Optionally update state with next_route\n",
    "    state[\"next_route\"] = next_route\n",
    "    return {\"next_route\": next_route}\n",
    "\n",
    "def get_route(state: State) -> str:\n",
    "    return state[\"route\"]\n",
    "\n",
    "def call_route_first_step(state: State):\n",
    "    # Debug print\n",
    "    print(f\"Checking for image in state: {bool(state.get('image') and len(state['image']) > 0)}\")\n",
    "    print(f\"Image processed status: {state.get('image_processed', False)}\")\n",
    "    \n",
    "    # Make sure to check the flag explicitly\n",
    "    image_processed = state.get(\"image_processed\", False)\n",
    "    # Only route to image analysis if we have an image AND it's not been processed\n",
    "    if state.get(\"image\") and len(state[\"image\"]) > 0 and not image_processed:\n",
    "        print(\"Routing to Image_Analysis\")\n",
    "        return {\"route\": \"Image_Analysis\"}\n",
    "    \n",
    "    # Otherwise proceed with regular routing\n",
    "    router_response = llm.with_structured_output(Route_First_Step).invoke(state[\"research_topic\"])\n",
    "    print('Image Analysis Done, now proceeding with regular routing, which is: ', router_response.step)\n",
    "    print(f\"Regular routing result: {router_response.step}\")\n",
    "    return {\"route\": router_response.step}\n",
    "\n",
    "def validate_state_transition(old_state: State, new_state: State):\n",
    "    required_fields = set(State.__annotations__.keys())\n",
    "    missing = required_fields - set(new_state.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing state updates for: {missing}\")\n",
    "    return True\n",
    "\n",
    "def after_image_analysis(state):\n",
    "    return {**state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemma3(state: State):\n",
    "    \"\"\"Process image using Gemma 3 model and save the response.\"\"\"\n",
    "    print(\"Inside call_gemma3 function\")\n",
    "    try:\n",
    "        image_path = state[\"image\"][0]\n",
    "        with open(\"C:\\Disk D\\Semester 4 2\\LLMs\\Financial_Agent\\Screenshot 2025-03-14 115707.png\", \"rb\") as f:\n",
    "            image_b64 = base64.b64encode(f.read()).decode()\n",
    "\n",
    "        assert len(image_b64) < 180_000, \\\n",
    "            \"To upload larger images, use the assets API (see docs)\"\n",
    "        \n",
    "        \n",
    "        # Use a simpler approach with the standard LLM as a fallback\n",
    "        try:\n",
    "            # Try Gemma API first\n",
    "            invoke_url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "            headers = {\n",
    "                \"Authorization\": \"Bearer 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\",\n",
    "                \"Accept\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": \"google/gemma-3-27b-it\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Describe what you see in this image. Focus on any charts, financial data, or technical analysis elements if present.\\n<img src=\\\"data:image/png;base64,{image_b64}\\\" />\"\n",
    "                    }\n",
    "                ],\n",
    "                \"max_tokens\": 512,\n",
    "                \"temperature\": 0.20,\n",
    "                \"top_p\": 0.70,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            print(\"Sending request to Gemma API...\")\n",
    "            gemma_response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "            gemma_response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            \n",
    "            data = gemma_response.json()\n",
    "            if \"choices\" in data and data[\"choices\"]:\n",
    "                full_response = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                print(f\"Got response from Gemma API: {full_response[:100]}...\")\n",
    "            else:\n",
    "                raise ValueError(\"No valid response content from Gemma API\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Gemma API error: {str(e)}. Using fallback method.\")\n",
    "            # Use local LLM as fallback\n",
    "            fallback_prompt = f\"Describe what you see in this image. The image appears to be a financial chart or technical analysis pattern graph from Investopedia. Please describe the patterns, trends, and elements visible in the chart.\"\n",
    "            full_response = llm.invoke(fallback_prompt).content\n",
    "            print(f\"Got fallback response: {full_response[:100]}...\")\n",
    "        \n",
    "        # Format the response as Markdown\n",
    "        markdown_response = f\"## Image Analysis Results\\n\\n{full_response}\"\n",
    "        \n",
    "        # Add the response to messages\n",
    "        updated_messages = state[\"messages\"] + [HumanMessage(content=markdown_response)]\n",
    "        state['image_processed'] = True\n",
    "        # Use the main LLM to determine the next route\n",
    "        route_prompt = \"Based on the image analysis of what appears to be a financial chart or technical analysis pattern, what should be the next step? Choose one: Web_query, Normal_query, Financial_Analysis, YouTube_Recommender\"\n",
    "        try:\n",
    "            next_route = llm.invoke(route_prompt).content.strip()\n",
    "            # Extract just the route name if there's additional text\n",
    "            for route in [\"Web_query\", \"Normal_query\", \"Financial_Analysis\", \"YouTube_Recommender\"]:\n",
    "                if route in next_route:\n",
    "                    next_route = route\n",
    "                    break\n",
    "            else:\n",
    "                next_route = \"Normal_query\"  # Default\n",
    "        except:\n",
    "            next_route = \"Normal_query\"  # Default on error\n",
    "            \n",
    "        print(f\"Setting next route to: {next_route}\")\n",
    "        \n",
    "        # Set image_processed flag to True and provide the next route\n",
    "        return {\n",
    "            \"running_summary\": markdown_response,\n",
    "            \"messages\": updated_messages,\n",
    "            \"image_processed\": True,  # This is correctly set here\n",
    "            \"route\": next_route,\n",
    "            # Make sure to include other needed state variables that should be preserved\n",
    "            \"research_topic\": state[\"research_topic\"],\n",
    "            \"search_query\": state.get(\"search_query\", \"\"),\n",
    "            \"web_research_results\": state.get(\"web_research_results\", []),\n",
    "            \"sources_gathered\": state.get(\"sources_gathered\", []),\n",
    "            \"research_loop_count\": state.get(\"research_loop_count\", 0),\n",
    "            \"image\": state[\"image\"]  # Important to maintain the image reference\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Similar update for the error case...\n",
    "        return {\n",
    "            # All the same state variables need to be preserved here as well\n",
    "            \"running_summary\": str(e),\n",
    "            \"messages\": state[\"messages\"] + [HumanMessage(content=str(e))],\n",
    "            \"image_processed\": True,\n",
    "            \"route\": \"Normal_query\",\n",
    "            \"research_topic\": state[\"research_topic\"],\n",
    "            \"search_query\": state.get(\"search_query\", \"\"),\n",
    "            \"web_research_results\": state.get(\"web_research_results\", []),\n",
    "            \"sources_gathered\": state.get(\"sources_gathered\", []),\n",
    "            \"research_loop_count\": state.get(\"research_loop_count\", 0),\n",
    "            \"image\": state[\"image\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_router():\n",
    "    builder = StateGraph(State)\n",
    "    \n",
    "    # Add all nodes\n",
    "    #final_router.add_node(\"route_first_step\", call_route_first_step)\n",
    "\n",
    "    #final_router.add_node('take_action', take_action)\n",
    "    #final_router.add_node('format_financial_analysis', format_financial_analysis)\n",
    "    #final_router.add_node('answer_normal_query', answer_normal_query)\n",
    "    #final_router.add_node('youtube_recommend', youtube_recommend)\n",
    "    #final_router.add_node(\"self_evaluate_final\", evaluate_response)\n",
    "    #final_router.add_node(\"evaluation_decision\", evaluation_decision)\n",
    "    # Add the new image processing node\n",
    "    #final_router.add_node(\"image_analysis\", call_gemma3)\n",
    "    \n",
    "    # Define connections\n",
    "    builder = StateGraph(State)\n",
    "    builder.add_node(\"generate_query\", generate_query)\n",
    "    builder.add_node(\"web_research\", web_research)\n",
    "    builder.add_node(\"summarize_sources\", summarize_sources)\n",
    "    builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n",
    "    builder.add_node(\"finalize_summary\", finalize_summary)\n",
    "    builder.add_edge(START, \"generate_query\")\n",
    "    builder.add_edge(\"generate_query\", \"web_research\")\n",
    "    builder.add_edge(\"web_research\", \"summarize_sources\")\n",
    "    builder.add_edge(\"summarize_sources\", \"reflect_on_summary\")\n",
    "    builder.add_conditional_edges(\"reflect_on_summary\", route_research)\n",
    "    builder.add_edge(\"finalize_summary\", END)\n",
    "    # Update conditional edges to include Image_Analysis route\n",
    "    \n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "# Update the create_initial_state function to better handle images\n",
    "def create_initial_state(user_query: str, image: list[str] = []) -> State:\n",
    "    return {\n",
    "        \"route\": None,\n",
    "        \"research_topic\": user_query,\n",
    "        \"search_query\": \"\",\n",
    "        \"web_research_results\": [],\n",
    "        \"sources_gathered\": [],\n",
    "        \"research_loop_count\": 0,\n",
    "        \"running_summary\": \"\",\n",
    "        \"image\": image,\n",
    "        \"messages\": [HumanMessage(content=user_query)]\n",
    "    }\n",
    "\n",
    "class Route_First_Step(BaseModel):\n",
    "    step: Literal['Web_query', 'Normal_query', 'Financial_Analysis', 'YouTube_Recommender'] = Field(\n",
    "        None,\n",
    "        description='Determine whether to perform a web search, answer a normal question, perform a financial analysis or recommend YouTube videos'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = create_initial_state(\"What is the latest news about Reliance Industries?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Error: Could not decode JSON from reflect_on_summary. Response was: ```json\n",
      "{\n",
      "    \"knowledge_gap\": \"The summary mentions expected EBITDA margin improvement but doesn't provide context on how this compares to analyst expectations or historical performance.  It also doesn't delve into specific drivers behind this improvement or potential challenges in sustaining it.\",\n",
      "    \"follow_up_query\": \"How does Reliance Industries' expected EBITDA margin of 18.2%-18.9% for Q3 compare to analyst forecasts and its own historical performance?\"\n",
      "}\n",
      "```\n",
      "Error: Could not decode JSON from reflect_on_summary. Response was: ```json\n",
      "{\n",
      "    \"knowledge_gap\": \"The summary mentions Reliance Retail's re-launch of Shein but doesn't provide details on the potential impact on Reliance Industries' financial performance. It would be helpful to understand how this strategic move might affect their revenue, profitability, and market share in the fast fashion segment.\",\n",
      "    \"follow_up_query\": \"How is Reliance Retail's re-launch of Shein expected to impact Reliance Industries' financial performance in the coming quarters?\"\n",
      "}\n",
      "```\n",
      "Error: Could not decode JSON from reflect_on_summary. Response was: ```json\n",
      "{\n",
      "    \"knowledge_gap\": \"The summary mentions Reliance Retail's re-launch of Shein but doesn't provide details about the financial implications of this move for Reliance Industries.  It would be helpful to understand how this partnership might impact Reliance's revenue, profitability, and market share.\",\n",
      "    \"follow_up_query\": \"What are the potential financial impacts of Reliance Retail's partnership with Shein on Reliance Industries' revenue, profits, and market position?\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "graph = update_router()\n",
    "summary = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Web Research Summary\n",
       "\n",
       "Reliance Industries is expected to report an EBITDA margin between 18.2% and 18.9% for its fiscal third quarter, showing improvement from the previous quarter.  Mukesh Ambani-led Reliance Retail has re-launched Shein in India, sparking a fast fashion war and challenging competitors like Trent's Zudio and Nykaa Fashion.  Despite potential competitive pressures, experts suggest that Shein's online-first model might face challenges in supply chain establishment and unit economics.  The combined market valuation of six of the top-10 most valued firms surged Rs 1,18,151.75 crore last week, with HDFC Bank and Bharti Airtel emerging as the biggest gainers, in-line with firm trend in equities.  Reliance Industries shares have been in focus as Shein re-enters India after a 2020 ban.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "### Sources:\n",
       "* Reliance Industries News - Reliance Industries Announcement, Latest ... : https://economictimes.indiatimes.com/reliance-industries-ltd/stocksupdate/companyid-13215.cms\n",
       "* Reliance Industries News - Reliance Industries Announcement, Latest ... : https://economictimes.indiatimes.com/reliance-industries-ltd/stocksupdate/companyid-13215.cms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(summary[\"running_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
